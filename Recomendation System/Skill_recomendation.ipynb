{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:06:50.849953Z",
     "iopub.status.busy": "2025-04-20T17:06:50.849203Z",
     "iopub.status.idle": "2025-04-20T17:06:54.318825Z",
     "shell.execute_reply": "2025-04-20T17:06:54.317749Z",
     "shell.execute_reply.started": "2025-04-20T17:06:50.84992Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-20T14:42:03.411196Z",
     "iopub.status.busy": "2025-04-20T14:42:03.410548Z",
     "iopub.status.idle": "2025-04-20T14:42:03.429853Z",
     "shell.execute_reply": "2025-04-20T14:42:03.429068Z",
     "shell.execute_reply.started": "2025-04-20T14:42:03.411169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "SKILLS_POOL = [\n",
    "    \"React\", \"JavaScript\", \"Node.js\", \"Python\", \"Django\", \"Flask\",\n",
    "    \"Graphic Design\", \"Illustration\", \"UI/UX Design\", \"Data Visualization\",\n",
    "    \"TypeScript\", \"Tailwind CSS\", \"Animation\", \"Machine Learning\", \"SQL\"\n",
    "]\n",
    "\n",
    "TONES = [\"creative\", \"professional\", \"technical\"]\n",
    "\n",
    "def generate_clients(num=5):\n",
    "    clients = []\n",
    "    for i in range(num):\n",
    "        num_skills = random.randint(2, 4)\n",
    "        skills = random.sample(SKILLS_POOL, num_skills)\n",
    "        tone = random.choice(TONES)\n",
    "        project_description = (\n",
    "            f\"{fake.sentence(nb_words=6, variable_nb_words=True)} \"\n",
    "            f\"requiring {', '.join(skills)} for a {tone} project\"\n",
    "        ).capitalize()\n",
    "        clients.append({\n",
    "            \"client_id\": i + 1,\n",
    "            \"project_description\": project_description,\n",
    "            \"skills_required\": skills,\n",
    "            \"budget\": round(random.uniform(300, 2000), 2),\n",
    "            \"timeline\": f\"{random.randint(5, 30)} days\",\n",
    "            \"created_at\": datetime(2025, 4, 20, random.randint(8, 18), random.randint(0, 59)).isoformat()\n",
    "        })\n",
    "    return clients\n",
    "\n",
    "# Generate 10 freelancers\n",
    "def generate_freelancers(num=10):\n",
    "    freelancers = []\n",
    "    for i in range(num):\n",
    "        num_skills = random.randint(3, 5)\n",
    "        skills = random.sample(SKILLS_POOL, num_skills)\n",
    "        tone = random.choice(TONES)\n",
    "        num_portfolio_items = random.randint(2, 3)\n",
    "        portfolio_text = [\n",
    "            f\"{fake.sentence(nb_words=5, variable_nb_words=True)} {skill.lower()} {tone} project\".capitalize()\n",
    "            for skill in random.sample(skills, num_portfolio_items)\n",
    "        ]\n",
    "        num_experiences = random.randint(1, 3)\n",
    "        experience = [\n",
    "            {\n",
    "                \"duration\": f\"{random.randint(1, 7)} years\",\n",
    "                \"experience_description\": f\"{fake.sentence(nb_words=6, variable_nb_words=True)} {skill.lower()}\".capitalize()\n",
    "            }\n",
    "            for skill in random.sample(skills, num_experiences)\n",
    "        ]\n",
    "        freelancers.append({\n",
    "            \"freelancer_id\": i + 1,\n",
    "            \"skills\": skills,\n",
    "            \"experience\": experience,\n",
    "            \"portfolio_text\": portfolio_text,\n",
    "            \"availability\": random.choice([True, False]),\n",
    "            \"avg_rating\": round(random.uniform(3.5, 5.0), 2) if random.random() > 0.2 else 0.0,\n",
    "            \"rate\": round(random.uniform(20, 100), 2),\n",
    "            \"created_at\": datetime(2025, 4, random.randint(1, 19), random.randint(8, 18)).isoformat(),\n",
    "            \"updated_at\": datetime(2025, 4, 20, random.randint(8, 18)).isoformat()\n",
    "        })\n",
    "    return freelancers\n",
    "\n",
    "# Generate 3 reviews\n",
    "def generate_reviews(num=3, freelancer_ids=range(1, 11)):\n",
    "    reviews = []\n",
    "    selected_freelancers = random.sample(list(freelancer_ids), num)\n",
    "    for i, freelancer_id in enumerate(selected_freelancers, 1):\n",
    "        tone = random.choice(TONES)\n",
    "        reviews.append({\n",
    "            \"review_id\": i,\n",
    "            \"freelancer_id\": freelancer_id,\n",
    "            \"review_text\": f\"{fake.sentence(nb_words=6, variable_nb_words=True)} {tone} work\".capitalize(),\n",
    "            \"rating\": round(random.uniform(4.0, 5.0), 2),\n",
    "            \"created_at\": datetime(2025, 4, random.randint(10, 19), random.randint(8, 18)).isoformat()\n",
    "        })\n",
    "    return reviews\n",
    "\n",
    "# Generate data\n",
    "clients = generate_clients(5)\n",
    "freelancers = generate_freelancers(10)\n",
    "reviews = generate_reviews(3)\n",
    "\n",
    "# Combine data\n",
    "data = {\n",
    "    \"clients\": clients,\n",
    "    \"freelancers\": freelancers,\n",
    "    \"reviews\": reviews\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"synthetic_data.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(\"Synthetic data generated and saved to 'synthetic_data.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T14:47:31.074915Z",
     "iopub.status.busy": "2025-04-20T14:47:31.074129Z",
     "iopub.status.idle": "2025-04-20T14:47:31.084722Z",
     "shell.execute_reply": "2025-04-20T14:47:31.084077Z",
     "shell.execute_reply.started": "2025-04-20T14:47:31.074888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "def load_synthetic_data(file_path: str = \"synthetic_data.json\") -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Load synthetic data from a JSON file and return clients, freelancers, and reviews as separate lists.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file (default: 'synthetic_data.json').\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "            - List of client dictionaries.\n",
    "            - List of freelancer dictionaries.\n",
    "            - List of review dictionaries.\n",
    "            \n",
    "    Raises:\n",
    "        FileNotFoundError: If the JSON file does not exist.\n",
    "        json.JSONDecodeError: If the JSON file is invalid.\n",
    "        KeyError: If expected keys ('clients', 'freelancers', 'reviews') are missing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the JSON file\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract clients, freelancers, and reviews\n",
    "        clients = data[\"clients\"]\n",
    "        freelancers = data[\"freelancers\"]\n",
    "        reviews = data[\"reviews\"]\n",
    "        \n",
    "        # Validate data\n",
    "        if not all([clients, freelancers, reviews]):\n",
    "            raise KeyError(\"JSON file missing required keys: 'clients', 'freelancers', or 'reviews'\")\n",
    "        \n",
    "        print(f\"Loaded {len(clients)} clients, {len(freelancers)} freelancers, and {len(reviews)} reviews from {file_path}\")\n",
    "        return clients, freelancers, reviews\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found\")\n",
    "        raise\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON format in '{file_path}': {str(e)}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing required keys in JSON data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load data into variables\n",
    "        clients_data, freelancers_data, reviews_data = load_synthetic_data()\n",
    "        \n",
    "        # Print sample data for verification\n",
    "        print(\"\\nSample Client:\")\n",
    "        print(json.dumps(clients_data[0], indent=2))\n",
    "        \n",
    "        print(\"\\nSample Freelancer:\")\n",
    "        print(json.dumps(freelancers_data[0], indent=2))\n",
    "        \n",
    "        print(\"\\nSample Review:\")\n",
    "        print(json.dumps(reviews_data[0], indent=2))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T15:16:30.609931Z",
     "iopub.status.busy": "2025-04-20T15:16:30.609598Z",
     "iopub.status.idle": "2025-04-20T15:16:33.92435Z",
     "shell.execute_reply": "2025-04-20T15:16:33.923496Z",
     "shell.execute_reply.started": "2025-04-20T15:16:30.609907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from typing import List, Dict, Any, Union\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "def preprocess_text(text: str, remove_stopwords: bool = False, lemmatize: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess a single text string by cleaning and normalizing it.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to preprocess.\n",
    "        remove_stopwords (bool): If True, remove stopwords (default: False to preserve tone words).\n",
    "        lemmatize (bool): If True, lemmatize words (default: True).\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned and normalized text.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, numbers, and extra spaces\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)  # Keep letters and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize spaces\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords (optional)\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize (optional)\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_synthetic_data(clients: List[Dict[str, Any]], \n",
    "                            freelancers: List[Dict[str, Any]], \n",
    "                            reviews: List[Dict[str, Any]], \n",
    "                            join_portfolio: bool = True,\n",
    "                            include_experience: bool = True) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Preprocess text fields in synthetic data (clients, freelancers, reviews).\n",
    "    \n",
    "    Args:\n",
    "        clients (List[Dict[str, Any]]): List of client dictionaries.\n",
    "        freelancers (List[Dict[str, Any]]): List of freelancer dictionaries.\n",
    "        reviews (List[Dict[str, Any]]): List of review dictionaries.\n",
    "        join_portfolio (bool): If True, join portfolio_text into a single string; else keep as list.\n",
    "        include_experience (bool): If True, preprocess experience_description and include in output.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "            - Preprocessed clients with cleaned project_description.\n",
    "            - Preprocessed freelancers with cleaned portfolio_text and experience.\n",
    "            - Preprocessed reviews with cleaned review_text.\n",
    "    \"\"\"\n",
    "    # Copy data to avoid modifying originals\n",
    "    clients_processed = clients.copy()\n",
    "    freelancers_processed = freelancers.copy()\n",
    "    reviews_processed = reviews.copy()\n",
    "    \n",
    "    # Preprocess clients (project_description)\n",
    "    for client in clients_processed:\n",
    "        client['project_description'] = preprocess_text(client['project_description'])\n",
    "    \n",
    "    # Preprocess freelancers (portfolio_text, experience)\n",
    "    for freelancer in freelancers_processed:\n",
    "        # Process portfolio_text\n",
    "        if join_portfolio:\n",
    "            # Join portfolio_text into a single string\n",
    "            joined_text = ' '.join(freelancer['portfolio_text'])\n",
    "            freelancer['portfolio_text'] = preprocess_text(joined_text)\n",
    "        else:\n",
    "            # Process each portfolio_text entry separately\n",
    "            freelancer['portfolio_text'] = [preprocess_text(text) for text in freelancer['portfolio_text']]\n",
    "        \n",
    "        # Process experience (if included)\n",
    "        if include_experience:\n",
    "            for exp in freelancer['experience']:\n",
    "                exp['experience_description'] = preprocess_text(exp['experience_description'])\n",
    "    \n",
    "    # Preprocess reviews (review_text)\n",
    "    for review in reviews_processed:\n",
    "        review['review_text'] = preprocess_text(review['review_text'])\n",
    "    \n",
    "    return clients_processed, freelancers_processed, reviews_processed\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load synthetic data\n",
    "        clients_data, freelancers_data, reviews_data = load_synthetic_data(\"synthetic_data.json\")\n",
    "        \n",
    "        # Preprocess data\n",
    "        clients_proc, freelancers_proc, reviews_proc = preprocess_synthetic_data(\n",
    "            clients_data,\n",
    "            freelancers_data,\n",
    "            reviews_data,\n",
    "            join_portfolio=True,  # Join portfolio_text for BERT embeddings\n",
    "            include_experience=True  # Include experience_description\n",
    "        )\n",
    "        \n",
    "        # Print samples for verification\n",
    "        print(\"\\nSample Preprocessed Client:\")\n",
    "        print(json.dumps(clients_proc[0], indent=2))\n",
    "        \n",
    "        print(\"\\nSample Preprocessed Freelancer:\")\n",
    "        print(json.dumps(freelancers_proc[0], indent=2))\n",
    "        \n",
    "        print(\"\\nSample Preprocessed Review:\")\n",
    "        print(json.dumps(reviews_proc[0], indent=2))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to preprocess data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Generate Client and Freelancer Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T15:37:19.839582Z",
     "iopub.status.busy": "2025-04-20T15:37:19.839294Z",
     "iopub.status.idle": "2025-04-20T15:37:44.585013Z",
     "shell.execute_reply": "2025-04-20T15:37:44.584189Z",
     "shell.execute_reply.started": "2025-04-20T15:37:19.839561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "def get_bert_embeddings(texts: List[str], model_name: str = 'distilbert-base-uncased', batch_size: int = 8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate BERT embeddings for a list of texts using a pre-trained model.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of text strings to embed.\n",
    "        model_name (str): Hugging Face model name (default: 'distilbert-base-uncased').\n",
    "        batch_size (int): Batch size for processing texts (default: 8).\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings (shape: [len(texts), 768]).\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    # Process texts in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize and encode\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Use [CLS] token embedding (first token)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        \n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Concatenate embeddings\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def get_tfidf_embeddings(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate TF-IDF embeddings for a list of texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of text strings to embed.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Array of TF-IDF vectors (sparse, converted to dense).\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    return tfidf_matrix.toarray()\n",
    "\n",
    "def extract_text_features(clients: List[Dict[str, Any]], \n",
    "                         freelancers: List[Dict[str, Any]], \n",
    "                         use_bert: bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract text features (BERT or TF-IDF embeddings) for project_description and portfolio_text.\n",
    "    \n",
    "    Args:\n",
    "        clients (List[Dict[str, Any]]): List of preprocessed client dictionaries.\n",
    "        freelancers (List[Dict[str, Any]]): List of preprocessed freelancer dictionaries.\n",
    "        use_bert (bool): If True, use BERT embeddings; else use TF-IDF (default: True).\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "            - Client embeddings (shape: [len(clients), 768] for BERT, or [len(clients), vocab_size] for TF-IDF).\n",
    "            - Freelancer embeddings (shape: [len(freelancers), 768] or [len(freelancers), vocab_size]).\n",
    "            - Cosine similarity matrix (shape: [len(clients), len(freelancers)]).\n",
    "    \"\"\"\n",
    "    # Extract text fields\n",
    "    client_texts = [client['project_description'] for client in clients]\n",
    "    freelancer_texts = [freelancer['portfolio_text'] for freelancer in freelancers]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    if use_bert:\n",
    "        # Combine texts for efficiency (single model load)\n",
    "        all_texts = client_texts + freelancer_texts\n",
    "        all_embeddings = get_bert_embeddings(all_texts)\n",
    "        \n",
    "        # Split embeddings\n",
    "        client_embeddings = all_embeddings[:len(client_texts)]\n",
    "        freelancer_embeddings = all_embeddings[len(client_texts):]\n",
    "    else:\n",
    "        # TF-IDF embeddings\n",
    "        all_texts = client_texts + freelancer_texts\n",
    "        all_embeddings = get_tfidf_embeddings(all_texts)\n",
    "        \n",
    "        # Split embeddings\n",
    "        client_embeddings = all_embeddings[:len(client_texts)]\n",
    "        freelancer_embeddings = all_embeddings[len(client_texts):]\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = cosine_similarity(client_embeddings, freelancer_embeddings)\n",
    "    \n",
    "    return client_embeddings, freelancer_embeddings, similarity_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Notes:  This should run every day, so new freelancer can be included in recommendation\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load synthetic data\n",
    "        clients_data, freelancers_data, reviews_data = load_synthetic_data(\"synthetic_data.json\")\n",
    "        \n",
    "        # Preprocess data (join_portfolio=True for single string)\n",
    "        clients_proc, freelancers_proc, reviews_proc = preprocess_synthetic_data(\n",
    "            clients_data,\n",
    "            freelancers_data,\n",
    "            reviews_data,\n",
    "            join_portfolio=True,\n",
    "            include_experience=False  # Exclude experience for now\n",
    "        )\n",
    "        \n",
    "        # Extract features\n",
    "        client_emb, freelancer_emb, similarity_matrix = extract_text_features(\n",
    "            clients_proc,\n",
    "            freelancers_proc,\n",
    "            use_bert=True  # Use BERT embeddings\n",
    "        )\n",
    "        \n",
    "        # Print shapes and sample results\n",
    "        print(f\"Client embeddings shape: {client_emb.shape}\")\n",
    "        print(f\"Freelancer embeddings shape: {freelancer_emb.shape}\")\n",
    "        print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "        \n",
    "        # Print sample similarity scores\n",
    "        print(\"\\nSample Cosine Similarity Scores (Client 1 vs. Freelancers):\")\n",
    "        for i, score in enumerate(similarity_matrix[0]):\n",
    "            print(f\"Freelancer {i+1}: {score:.4f}\")\n",
    "        \n",
    "        # Print sample embedding (first 5 dimensions for brevity)\n",
    "        print(\"\\nSample Client 1 Embedding (first 5 dimensions):\")\n",
    "        print(client_emb[0][:5])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract features: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Find Similarity From a Client Data and All Freelancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:11:27.9733Z",
     "iopub.status.busy": "2025-04-20T16:11:27.972972Z",
     "iopub.status.idle": "2025-04-20T16:11:28.364063Z",
     "shell.execute_reply": "2025-04-20T16:11:28.363139Z",
     "shell.execute_reply.started": "2025-04-20T16:11:27.973276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def get_bert_embedding(text: str, model_name: str = 'distilbert-base-uncased') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate BERT embedding for a single text string.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to embed.\n",
    "        model_name (str): Hugging Face model name (default: 'distilbert-base-uncased').\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Embedding vector (shape: [768]).\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and encode\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get embedding\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use [CLS] token embedding\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    return embedding[0]  # Shape: [768]\n",
    "\n",
    "def compute_client_similarity(client_data: Dict[str, Any], \n",
    "                             freelancer_embeddings_file: str = \"freelancer_embeddings.npy\",\n",
    "                             freelancers_data: List[Dict[str, Any]] = None) -> List[float]:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between a client's project_description and each freelancer's portfolio_text.\n",
    "    \n",
    "    Args:\n",
    "        client_data (Dict[str, Any]): Client data with 'project_description' key.\n",
    "        freelancer_embeddings_file (str): Path to precomputed freelancer embeddings (default: 'freelancer_embeddings.npy').\n",
    "        freelancers_data (List[Dict[str, Any]]): Preprocessed freelancer data (optional, used if embeddings file is missing).\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: Cosine similarity scores for each freelancer (ordered by freelancer_id).\n",
    "        \n",
    "    Raises:\n",
    "        KeyError: If 'project_description' is missing in client_data.\n",
    "        FileNotFoundError: If freelancer_embeddings_file is missing and freelancers_data is not provided.\n",
    "    \"\"\"\n",
    "    # Validate client data\n",
    "    if 'project_description' not in client_data:\n",
    "        raise KeyError(\"client_data must contain 'project_description'\")\n",
    "    \n",
    "    # Preprocess client project_description\n",
    "    client_text = preprocess_text(client_data['project_description'])\n",
    "    \n",
    "    # Generate client embedding\n",
    "    client_embedding = get_bert_embedding(client_text).reshape(1, -1)  # Shape: [1, 768]\n",
    "    \n",
    "    # Load or compute freelancer embeddings\n",
    "    try:\n",
    "        # Try loading precomputed embeddings\n",
    "        freelancer_embeddings = np.load(freelancer_embeddings_file)\n",
    "    except FileNotFoundError:\n",
    "        if freelancers_data is None:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Freelancer embeddings file '{freelancer_embeddings_file}' not found, \"\n",
    "                \"and freelancers_data not provided\"\n",
    "            )\n",
    "        # Compute embeddings on-the-fly\n",
    "        freelancer_texts = [freelancer['portfolio_text'] for freelancer in freelancers_data]\n",
    "        freelancer_embeddings = np.vstack([\n",
    "            get_bert_embedding(text) for text in freelancer_texts\n",
    "        ])\n",
    "        # Save for future use\n",
    "        np.save(freelancer_embeddings_file, freelancer_embeddings)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_scores = cosine_similarity(client_embedding, freelancer_embeddings)[0]\n",
    "    \n",
    "    return similarity_scores.tolist()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load synthetic data\n",
    "        clients_data, freelancers_data, reviews_data = load_synthetic_data(\"synthetic_data.json\")\n",
    "        \n",
    "        # Preprocess data (join_portfolio=True for single string)\n",
    "        _, freelancers_proc, _ = preprocess_synthetic_data(\n",
    "            clients_data,\n",
    "            freelancers_data,\n",
    "            reviews_data,\n",
    "            join_portfolio=True,\n",
    "            include_experience=False\n",
    "        )\n",
    "        \n",
    "        # Example: Assume precomputed freelancer embeddings exist\n",
    "        # (Run extract_text_features.py first to generate freelancer_embeddings.npy)\n",
    "        \n",
    "        # New client data\n",
    "        new_client = {\n",
    "            \"project_description\": \"REACT\"\n",
    "        }\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        similarity_scores = compute_client_similarity(\n",
    "            client_data=new_client,\n",
    "            freelancer_embeddings_file=\"freelancer_embeddings.npy\",\n",
    "            freelancers_data=freelancers_proc  # Fallback if file missing\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nCosine Similarity Scores for New Client vs. Freelancers:\")\n",
    "        for i, score in enumerate(similarity_scores, 1):\n",
    "            print(f\"Freelancer {i}: {score:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to compute similarities: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:01:00.843439Z",
     "iopub.status.busy": "2025-04-20T17:01:00.842825Z",
     "iopub.status.idle": "2025-04-20T17:01:38.324751Z",
     "shell.execute_reply": "2025-04-20T17:01:38.323427Z",
     "shell.execute_reply.started": "2025-04-20T17:01:00.843414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# from typing import List, Dict, Any, Tuple\n",
    "# import torch\n",
    "# from faker import Faker\n",
    "\n",
    "# # Initialize Faker for synthetic data\n",
    "# fake = Faker()\n",
    "\n",
    "# def generate_synthetic_tone_data(num_samples_per_class: int = 100) -> Tuple[List[str], List[int]]:\n",
    "#     \"\"\"\n",
    "#     Generate synthetic labeled data for tone classification (creative, professional, technical).\n",
    "    \n",
    "#     Args:\n",
    "#         num_samples_per_class (int): Number of samples per tone class (default: 100).\n",
    "        \n",
    "#     Returns:\n",
    "#         Tuple[List[str], List[int]]: Texts and corresponding labels (0: creative, 1: professional, 2: technical).\n",
    "#     \"\"\"\n",
    "#     tones = ['creative', 'professional', 'technical']\n",
    "#     texts = []\n",
    "#     labels = []\n",
    "    \n",
    "#     for tone_idx, tone in enumerate(tones):\n",
    "#         for _ in range(num_samples_per_class):\n",
    "#             if tone == 'creative':\n",
    "#                 text = f\"{fake.sentence(nb_words=6)} vibrant {tone} design project\"\n",
    "#             elif tone == 'professional':\n",
    "#                 text = f\"{fake.sentence(nb_words=6)} reliable {tone} service delivery\"\n",
    "#             else:  # technical\n",
    "#                 text = f\"{fake.sentence(nb_words=6)} scalable {tone} system implementation\"\n",
    "#             texts.append(text)\n",
    "#             labels.append(tone_idx)\n",
    "    \n",
    "#     return texts, labels\n",
    "\n",
    "# def train_tone_classifier(texts: List[str], labels: List[int], use_bert: bool = True, model_name: str = 'distilbert-base-uncased') -> Any:\n",
    "#     \"\"\"\n",
    "#     Train a tone classifier (BERT or Logistic Regression) on labeled texts.\n",
    "    \n",
    "#     Args:\n",
    "#         texts (List[str]): Preprocessed texts for training.\n",
    "#         labels (List[int]): Labels (0: creative, 1: professional, 2: technical).\n",
    "#         use_bert (bool): If True, use BERT; else use Logistic Regression with TF-IDF (default: True).\n",
    "#         model_name (str): Hugging Face model name for BERT (default: 'distilbert-base-uncased').\n",
    "        \n",
    "#     Returns:\n",
    "#         Any: Trained model (BERT Trainer or LogisticRegression).\n",
    "#     \"\"\"\n",
    "#     if use_bert:\n",
    "#         # Tokenize texts\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         encodings = tokenizer(\n",
    "#             texts,\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=128,\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "        \n",
    "#         # Create dataset\n",
    "#         class ToneDataset(torch.utils.data.Dataset):\n",
    "#             def __init__(self, encodings, labels):\n",
    "#                 self.encodings = encodings\n",
    "#                 self.labels = labels\n",
    "            \n",
    "#             def __getitem__(self, idx):\n",
    "#                 item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "#                 item['labels'] = torch.tensor(self.labels[idx])\n",
    "#                 return item\n",
    "            \n",
    "#             def __len__(self):\n",
    "#                 return len(self.labels)\n",
    "        \n",
    "#         dataset = ToneDataset(encodings, labels)\n",
    "        \n",
    "#         # Split into train and validation (80-20)\n",
    "#         train_size = int(0.8 * len(dataset))\n",
    "#         train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "        \n",
    "#         # Initialize model\n",
    "#         model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "        \n",
    "#         # Training arguments\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=\"./tone_classifier\",\n",
    "#             num_train_epochs=3,\n",
    "#             per_device_train_batch_size=8,\n",
    "#             per_device_eval_batch_size=8,\n",
    "#             warmup_steps=10,\n",
    "#             weight_decay=0.01,\n",
    "#             logging_dir=\"./logs\",\n",
    "#             logging_steps=10,\n",
    "#             eval_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             load_best_model_at_end=True\n",
    "#         )\n",
    "        \n",
    "#         # Initialize trainer\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=train_dataset,\n",
    "#             eval_dataset=val_dataset,\n",
    "#             compute_metrics=lambda p: {\n",
    "#                 \"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1)),\n",
    "#                 \"f1\": f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='weighted')\n",
    "#             }\n",
    "#         )\n",
    "        \n",
    "#         # Train\n",
    "#         trainer.train()\n",
    "#         return trainer\n",
    "    \n",
    "#     else:\n",
    "#         # TF-IDF with Logistic Regression\n",
    "#         vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "#         X = vectorizer.fit_transform(texts)\n",
    "#         model = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "#         model.fit(X, labels)\n",
    "#         return model, vectorizer\n",
    "\n",
    "# def predict_tone(texts: List[str], model: Any, use_bert: bool = True, tokenizer: Any = None, vectorizer: Any = None) -> List[int]:\n",
    "#     \"\"\"\n",
    "#     Predict tone labels for a list of texts.\n",
    "    \n",
    "#     Args:\n",
    "#         texts (List[str]): Preprocessed texts to classify.\n",
    "#         model: Trained model (BERT Trainer or LogisticRegression).\n",
    "#         use_bert (bool): If True, use BERT model; else use Logistic Regression (default: True).\n",
    "#         tokenizer: BERT tokenizer (required if use_bert=True).\n",
    "#         vectorizer: TF-IDF vectorizer (required if use_bert=False).\n",
    "        \n",
    "#     Returns:\n",
    "#         List[int]: Predicted labels (0: creative, 1: professional, 2: technical).\n",
    "#     \"\"\"\n",
    "#     if use_bert:\n",
    "#         encodings = tokenizer(\n",
    "#             texts,\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=128,\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "#         dataset = torch.utils.data.TensorDataset(\n",
    "#             encodings['input_ids'],\n",
    "#             encodings['attention_mask']\n",
    "#         )\n",
    "#         predictions = model.predict(dataset)\n",
    "#         return np.argmax(predictions, axis=1).tolist()\n",
    "#     else:\n",
    "#         X = vectorizer.transform(texts)\n",
    "#         return model.predict(X).tolist()\n",
    "\n",
    "# def perform_tone_analysis(clients: List[Dict[str, Any]], \n",
    "#                          freelancers: List[Dict[str, Any]], \n",
    "#                          reviews: List[Dict[str, Any]], \n",
    "#                          use_bert: bool = True) -> Tuple[List[str], List[str], np.ndarray]:\n",
    "#     \"\"\"\n",
    "#     Perform tone analysis on review_text, portfolio_text, and project_description.\n",
    "    \n",
    "#     Args:\n",
    "#         clients (List[Dict[str, Any]]): Preprocessed client dictionaries.\n",
    "#         freelancers (List[Dict[str, Any]]): Preprocessed freelancer dictionaries.\n",
    "#         reviews (List[Dict[str, Any]]): Preprocessed review dictionaries.\n",
    "#         use_bert (bool): If True, use BERT classifier; else use Logistic Regression (default: True).\n",
    "        \n",
    "#     Returns:\n",
    "#         Tuple[List[str], List[str], np.ndarray]:\n",
    "#             - Client tones (e.g., ['professional', 'creative', ...]).\n",
    "#             - Freelancer tones (e.g., ['technical', 'professional', ...]).\n",
    "#             - Tone match matrix (shape: [len(clients), len(freelancers)], 1 if tones match, 0 otherwise).\n",
    "#     \"\"\"\n",
    "#     # Generate synthetic training data\n",
    "#     synthetic_texts, synthetic_labels = generate_synthetic_tone_data(num_samples_per_class=100)\n",
    "    \n",
    "#     # Add real review_text (assume manually labeled for demo)\n",
    "#     review_texts = [review['review_text'] for review in reviews]\n",
    "#     # Example: Manually label the 3 reviews (in practice, label manually or use external data)\n",
    "#     review_labels = [1, 0, 2]  # e.g., professional, creative, technical\n",
    "#     if len(review_texts) != len(review_labels):\n",
    "#         raise ValueError(\"Number of review texts and labels must match\")\n",
    "    \n",
    "#     # Combine training data\n",
    "#     train_texts = synthetic_texts + review_texts\n",
    "#     train_labels = synthetic_labels + review_labels\n",
    "    \n",
    "#     # Train classifier\n",
    "#     if use_bert:\n",
    "#         trainer = train_tone_classifier(train_texts, train_labels, use_bert=True)\n",
    "#         tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "#         model = trainer\n",
    "#         vectorizer = None\n",
    "#     else:\n",
    "#         model, vectorizer = train_tone_classifier(train_texts, train_labels, use_bert=False)\n",
    "#         tokenizer = None\n",
    "    \n",
    "#     # Predict tones\n",
    "#     tone_map = {0: 'creative', 1: 'professional', 2: 'technical'}\n",
    "    \n",
    "#     # Client tones (project_description)\n",
    "#     client_texts = [client['project_description'] for client in clients]\n",
    "#     client_tone_ids = predict_tone(client_texts, model, use_bert, tokenizer, vectorizer)\n",
    "#     client_tones = [tone_map[tid] for tid in client_tone_ids]\n",
    "    \n",
    "#     # Freelancer tones (portfolio_text, fallback to reviews if available)\n",
    "#     freelancer_tones = []\n",
    "#     for freelancer in freelancers:\n",
    "#         # Use portfolio_text (all freelancers have it)\n",
    "#         portfolio_text = freelancer['portfolio_text']\n",
    "#         portfolio_tone_id = predict_tone([portfolio_text], model, use_bert, tokenizer, vectorizer)[0]\n",
    "        \n",
    "#         # Check for reviews (optional augmentation)\n",
    "#         freelancer_reviews = [r['review_text'] for r in reviews if r['freelancer_id'] == freelancer['freelancer_id']]\n",
    "#         if freelancer_reviews:\n",
    "#             review_tone_ids = predict_tone(freelancer_reviews, model, use_bert, tokenizer, vectorizer)\n",
    "#             # Aggregate: majority vote or use portfolio_text if tied\n",
    "#             tone_counts = np.bincount(review_tone_ids + [portfolio_tone_id], minlength=3)\n",
    "#             tone_id = np.argmax(tone_counts)\n",
    "#         else:\n",
    "#             tone_id = portfolio_tone_id\n",
    "        \n",
    "#         freelancer_tones.append(tone_map[tone_id])\n",
    "    \n",
    "#     # Compute tone match matrix\n",
    "#     tone_match_matrix = np.zeros((len(clients), len(freelancers)))\n",
    "#     for i, client_tone in enumerate(client_tones):\n",
    "#         for j, freelancer_tone in enumerate(freelancer_tones):\n",
    "#             tone_match_matrix[i, j] = 1 if client_tone == freelancer_tone else 0\n",
    "    \n",
    "#     return client_tones, freelancer_tones, tone_match_matrix\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         # Load synthetic data\n",
    "#         clients_data, freelancers_data, reviews_data = load_synthetic_data(\"synthetic_data.json\")\n",
    "        \n",
    "#         # Preprocess data (join_portfolio=True for single string)\n",
    "#         clients_proc, freelancers_proc, reviews_proc = preprocess_synthetic_data(\n",
    "#             clients_data,\n",
    "#             freelancers_data,\n",
    "#             reviews_data,\n",
    "#             join_portfolio=True,\n",
    "#             include_experience=False\n",
    "#         )\n",
    "        \n",
    "#         # Perform tone analysis\n",
    "#         client_tones, freelancer_tones, tone_match_matrix = perform_tone_analysis(\n",
    "#             clients_proc,\n",
    "#             freelancers_proc,\n",
    "#             reviews_proc,\n",
    "#             use_bert=True\n",
    "#         )\n",
    "        \n",
    "#         # Print results\n",
    "#         print(\"\\nClient Tones:\")\n",
    "#         for i, tone in enumerate(client_tones, 1):\n",
    "#             print(f\"Client {i}: {tone}\")\n",
    "        \n",
    "#         print(\"\\nFreelancer Tones:\")\n",
    "#         for i, tone in enumerate(freelancer_tones, 1):\n",
    "#             print(f\"Freelancer {i}: {tone}\")\n",
    "        \n",
    "#         print(\"\\nTone Match Matrix (Client vs. Freelancer):\")\n",
    "#         for i, row in enumerate(tone_match_matrix, 1):\n",
    "#             print(f\"Client {i}: {row.tolist()}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to perform tone analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:29:40.425986Z",
     "iopub.status.busy": "2025-04-20T16:29:40.425624Z",
     "iopub.status.idle": "2025-04-20T16:29:40.45864Z",
     "shell.execute_reply": "2025-04-20T16:29:40.457847Z",
     "shell.execute_reply.started": "2025-04-20T16:29:40.425963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "from fuzzywuzzy import fuzz\n",
    "import itertools\n",
    "\n",
    "# Define skill universe (from data generation script, April 20, 2025)\n",
    "SKILLS_POOL = [\n",
    "    \"React\", \"JavaScript\", \"Node.js\", \"Python\", \"Django\", \"Flask\",\n",
    "    \"Graphic Design\", \"Illustration\", \"UI/UX Design\", \"Data Visualization\",\n",
    "    \"TypeScript\", \"Tailwind CSS\", \"Animation\", \"Machine Learning\", \"SQL\"\n",
    "]\n",
    "\n",
    "def extract_skill_features(clients: List[Dict[str, Any]], \n",
    "                          freelancers: List[Dict[str, Any]], \n",
    "                          use_one_hot: bool = True) -> Tuple[Union[np.ndarray, List[set]], Union[np.ndarray, List[set]], List[str]]:\n",
    "    \"\"\"\n",
    "    Extract skill features as one-hot encoded vectors or skill sets.\n",
    "    \n",
    "    Args:\n",
    "        clients (List[Dict[str, Any]]): Client dictionaries with 'skills_required'.\n",
    "        freelancers (List[Dict[str, Any]]): Freelancer dictionaries with 'skills'.\n",
    "        use_one_hot (bool): If True, use one-hot encoding; else return skill sets (default: True).\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[Union[np.ndarray, List[set]], Union[np.ndarray, List[set]], List[str]]:\n",
    "            - Client skill features (array of shape [num_clients, num_skills] or list of sets).\n",
    "            - Freelancer skill features (array of shape [num_freelancers, num_skills] or list of sets).\n",
    "            - Skill labels (list of skill names, only for one-hot encoding).\n",
    "    \"\"\"\n",
    "    # Extract skill lists\n",
    "    client_skills = [client['skills_required'] for client in clients]\n",
    "    freelancer_skills = [freelancer['skills'] for freelancer in freelancers]\n",
    "    \n",
    "    if use_one_hot:\n",
    "        # Initialize MultiLabelBinarizer with predefined skill universe\n",
    "        mlb = MultiLabelBinarizer(classes=SKILLS_POOL)\n",
    "        \n",
    "        # Fit and transform skills\n",
    "        client_features = mlb.fit_transform(client_skills)\n",
    "        freelancer_features = mlb.transform(freelancer_skills)\n",
    "        \n",
    "        return client_features, freelancer_features, mlb.classes_.tolist()\n",
    "    else:\n",
    "        # Return skill sets for exact matching\n",
    "        client_features = [set(skills) for skills in client_skills]\n",
    "        freelancer_features = [set(skills) for skills in freelancer_skills]\n",
    "        return client_features, freelancer_features, SKILLS_POOL\n",
    "\n",
    "def compute_skill_similarity(clients: List[Dict[str, Any]], \n",
    "                            freelancers: List[Dict[str, Any]], \n",
    "                            use_one_hot: bool = True, \n",
    "                            use_fuzzy: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute skill similarity between clients and freelancers using Jaccard similarity.\n",
    "    \n",
    "    Args:\n",
    "        clients (List[Dict[str, Any]]): Client dictionaries with 'skills_required'.\n",
    "        freelancers (List[Dict[str, Any]]): Freelancer dictionaries with 'skills'.\n",
    "        use_one_hot (bool): If True, use one-hot encoded vectors; else use set-based Jaccard (default: True).\n",
    "        use_fuzzy (bool): If True, use fuzzy matching for skills (default: False).\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Similarity matrix (shape: [num_clients, num_freelancers]).\n",
    "    \"\"\"\n",
    "    # Extract skill features\n",
    "    client_features, freelancer_features, skill_labels = extract_skill_features(clients, freelancers, use_one_hot)\n",
    "    \n",
    "    # Initialize similarity matrix\n",
    "    similarity_matrix = np.zeros((len(clients), len(freelancers)))\n",
    "    \n",
    "    if use_one_hot and not use_fuzzy:\n",
    "        # Jaccard similarity on one-hot vectors\n",
    "        for i, client_vec in enumerate(client_features):\n",
    "            for j, freelancer_vec in enumerate(freelancer_features):\n",
    "                intersection = np.sum(client_vec & freelancer_vec)\n",
    "                union = np.sum(client_vec | freelancer_vec)\n",
    "                similarity_matrix[i, j] = intersection / union if union > 0 else 0.0\n",
    "    else:\n",
    "        # Set-based Jaccard similarity\n",
    "        for i, client_skills in enumerate(client_features):\n",
    "            for j, freelancer_skills in enumerate(freelancer_features):\n",
    "                if use_fuzzy:\n",
    "                    # Fuzzy matching: Compute max similarity for each skill pair\n",
    "                    intersection = 0\n",
    "                    for c_skill, f_skill in itertools.product(client_skills, freelancer_skills):\n",
    "                        score = fuzz.ratio(c_skill.lower(), f_skill.lower()) / 100.0\n",
    "                        if score > 0.9:  # Threshold for match\n",
    "                            intersection += 1\n",
    "                    union = len(client_skills) + len(freelancer_skills) - intersection\n",
    "                    similarity_matrix[i, j] = intersection / union if union > 0 else 0.0\n",
    "                else:\n",
    "                    # Exact matching\n",
    "                    intersection = len(client_skills & freelancer_skills)\n",
    "                    union = len(client_skills | freelancer_skills)\n",
    "                    similarity_matrix[i, j] = intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load synthetic data\n",
    "        clients_data, freelancers_data, reviews_data = load_synthetic_data(\"synthetic_data.json\")\n",
    "        \n",
    "        # Extract skill features\n",
    "        client_features, freelancer_features, skill_labels = extract_skill_features(\n",
    "            clients_data,\n",
    "            freelancers_data,\n",
    "            use_one_hot=True\n",
    "        )\n",
    "        \n",
    "        # Compute skill similarity\n",
    "        similarity_matrix = compute_skill_similarity(\n",
    "            clients_data,\n",
    "            freelancers_data,\n",
    "            use_one_hot=True,\n",
    "            use_fuzzy=False\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nSkill Labels:\")\n",
    "        print(skill_labels)\n",
    "        \n",
    "        print(\"\\nSample Client Skill Vector (Client 1):\")\n",
    "        print(client_features[0])\n",
    "        \n",
    "        print(\"\\nSample Freelancer Skill Vector (Freelancer 1):\")\n",
    "        print(freelancer_features[0])\n",
    "        \n",
    "        print(\"\\nSkill Similarity Matrix (Client vs. Freelancer):\")\n",
    "        for i, row in enumerate(similarity_matrix, 1):\n",
    "            print(f\"Client {i}: {[f'{x:.4f}' for x in row]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process skills: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similarity for a client with each freelancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:47:13.72551Z",
     "iopub.status.busy": "2025-04-20T16:47:13.725221Z",
     "iopub.status.idle": "2025-04-20T16:47:13.736129Z",
     "shell.execute_reply": "2025-04-20T16:47:13.735219Z",
     "shell.execute_reply.started": "2025-04-20T16:47:13.72549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from fuzzywuzzy import fuzz\n",
    "import itertools\n",
    "\n",
    "def compute_client_skill_similarity(client_skills: List[str], \n",
    "                                   freelancers_data: List[Dict[str, Any]] = None, \n",
    "                                   use_fuzzy: bool = False) -> List[float]:\n",
    "    \"\"\"\n",
    "    Compute Jaccard similarity between a client's skills and each freelancer's skills.\n",
    "    \n",
    "    Args:\n",
    "        client_skills (List[str]): List of client skills (e.g., ['React', 'JavaScript']).\n",
    "        freelancers_data (List[Dict[str, Any]]): Preprocessed freelancer data (optional, loaded if not provided).\n",
    "        use_fuzzy (bool): If True, use fuzzy matching for skills (default: False).\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: Jaccard similarity scores for each freelancer (ordered by freelancer_id).\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If client_skills is empty.\n",
    "        FileNotFoundError: If freelancers_data is not provided and synthetic_data.json is missing.\n",
    "    \"\"\"\n",
    "    # Validate client skills\n",
    "    if not client_skills:\n",
    "        raise ValueError(\"client_skills cannot be empty\")\n",
    "    \n",
    "    # Load freelancer data if not provided\n",
    "    if freelancers_data is None:\n",
    "        try:\n",
    "            _, freelancers_data, _ = load_synthetic_data(\"synthetic_data.json\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"synthetic_data.json not found and freelancers_data not provided\")\n",
    "    \n",
    "    # Convert client skills to set\n",
    "    client_skills_set = set(client_skills)\n",
    "    \n",
    "    # Initialize similarity scores\n",
    "    similarity_scores = []\n",
    "    \n",
    "    # Compute similarity for each freelancer\n",
    "    for freelancer in freelancers_data:\n",
    "        freelancer_skills = set(freelancer['skills'])\n",
    "        \n",
    "        if use_fuzzy:\n",
    "            # Fuzzy matching: Compute max similarity for each skill pair\n",
    "            intersection = 0\n",
    "            for c_skill, f_skill in itertools.product(client_skills, freelancer_skills):\n",
    "                score = fuzz.ratio(c_skill.lower(), f_skill.lower()) / 100.0\n",
    "                if score > 0.9:  # Threshold for match\n",
    "                    intersection += 1\n",
    "            union = len(client_skills) + len(freelancer_skills) - intersection\n",
    "            similarity = intersection / union if union > 0 else 0.0\n",
    "        else:\n",
    "            # Exact matching\n",
    "            intersection = len(client_skills_set & freelancer_skills)\n",
    "            union = len(client_skills_set | freelancer_skills)\n",
    "            similarity = intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        similarity_scores.append(similarity)\n",
    "    \n",
    "    return similarity_scores\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load synthetic data\n",
    "        _, freelancers_data, _ = load_synthetic_data(\"synthetic_data.json\")\n",
    "        \n",
    "        # Example client skills\n",
    "        new_client_skills = [\"React\", \"JavaScript\", \"UI/UX Design\"]\n",
    "        \n",
    "        # Compute skill similarity\n",
    "        similarity_scores = compute_client_skill_similarity(\n",
    "            client_skills=new_client_skills,\n",
    "            freelancers_data=freelancers_data,\n",
    "            use_fuzzy=False\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nSkill Similarity Scores for Client vs. Freelancers:\")\n",
    "        for i, score in enumerate(similarity_scores, 1):\n",
    "            print(f\"Freelancer {i}: {score:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to compute skill similarities: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating final score from a client for each freelancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:02:21.368751Z",
     "iopub.status.busy": "2025-04-20T17:02:21.368447Z",
     "iopub.status.idle": "2025-04-20T17:02:21.773937Z",
     "shell.execute_reply": "2025-04-20T17:02:21.773093Z",
     "shell.execute_reply.started": "2025-04-20T17:02:21.368728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def compute_final_score(client_data: Dict[str, Any], \n",
    "                       freelancers_data: List[Dict[str, Any]] = None,\n",
    "                       freelancer_embeddings_file: str = \"freelancer_embeddings.npy\",\n",
    "                       weights: Dict[str, float] = None) -> List[float]:\n",
    "    \"\"\"\n",
    "    Compute final score for a client against each freelancer based on text similarity,\n",
    "    skill similarity, and average rating.\n",
    "    \n",
    "    Args:\n",
    "        client_data (Dict[str, Any]): Client data with 'project_description' and 'skills_required'.\n",
    "        freelancers_data (List[Dict[str, Any]]): Preprocessed freelancer data (optional, loaded if not provided).\n",
    "        freelancer_embeddings_file (str): Path to precomputed freelancer embeddings (default: 'freelancer_embeddings.npy').\n",
    "        weights (Dict[str, float]): Weights for scoring components (default: {'skill': 0.4, 'text': 0.4, 'rating': 0.2}).\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: Final scores for each freelancer (ordered by freelancer_id).\n",
    "        \n",
    "    Raises:\n",
    "        KeyError: If required client_data keys are missing.\n",
    "        FileNotFoundError: If freelancers_data or embeddings file is missing.\n",
    "        ValueError: If weights are invalid.\n",
    "    \"\"\"\n",
    "    # Validate client data\n",
    "    required_keys = ['project_description', 'skills_required']\n",
    "    missing_keys = [key for key in required_keys if key not in client_data]\n",
    "    if missing_keys:\n",
    "        raise KeyError(f\"client_data missing required keys: {missing_keys}\")\n",
    "    \n",
    "    # Load freelancer data if not provided\n",
    "    if freelancers_data is None:\n",
    "        try:\n",
    "            _, freelancers_data, _ = load_synthetic_data(\"synthetic_data.json\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"synthetic_data.json not found and freelancers_data not provided\")\n",
    "    \n",
    "    # Set default weights\n",
    "    if weights is None:\n",
    "        weights = {'skill': 0.4, 'text': 0.4, 'rating': 0.2}\n",
    "    \n",
    "    # Validate weights\n",
    "    if not all(k in weights for k in ['skill', 'text', 'rating']):\n",
    "        raise ValueError(\"weights must include 'skill', 'text', and 'rating'\")\n",
    "    if not abs(sum(weights.values()) - 1.0) < 1e-6:\n",
    "        raise ValueError(\"weights must sum to 1.0\")\n",
    "    if any(w < 0 for w in weights.values()):\n",
    "        raise ValueError(\"weights must be non-negative\")\n",
    "    \n",
    "    # Compute text similarity\n",
    "    text_similarities = compute_client_similarity(\n",
    "        client_data=client_data,\n",
    "        freelancer_embeddings_file=freelancer_embeddings_file,\n",
    "        freelancers_data=freelancers_data\n",
    "    )\n",
    "    \n",
    "    # Compute skill similarity\n",
    "    skill_similarities = compute_client_skill_similarity(\n",
    "        client_skills=client_data['skills_required'],\n",
    "        freelancers_data=freelancers_data,\n",
    "        use_fuzzy=False\n",
    "    )\n",
    "    \n",
    "    # Extract and normalize average ratings (assume max rating is 5.0)\n",
    "    ratings = [freelancer['avg_rating'] / 5.0 for freelancer in freelancers_data]\n",
    "    \n",
    "    # Compute final scores\n",
    "    final_scores = [\n",
    "        weights['skill'] * skill_sim + weights['text'] * text_sim + weights['rating'] * rating\n",
    "        for skill_sim, text_sim, rating in zip(skill_similarities, text_similarities, ratings)\n",
    "    ]\n",
    "    \n",
    "    return final_scores\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load synthetic data\n",
    "        clients_data, freelancers_data, _ = load_synthetic_data(\"synthetic_data.json\")\n",
    "        \n",
    "        # Example client data\n",
    "        new_client = {\n",
    "            \"project_description\": \"Build a React e-commerce website with modern UI/UX\",\n",
    "            \"skills_required\": [\"React\", \"JavaScript\", \"UI/UX Design\"]\n",
    "        }\n",
    "\n",
    "        new_client = clients_data[0]\n",
    "        \n",
    "        # Compute final scores\n",
    "        final_scores = compute_final_score(\n",
    "            client_data=new_client,\n",
    "            freelancers_data=freelancers_data,\n",
    "            freelancer_embeddings_file=\"freelancer_embeddings.npy\",\n",
    "            weights={'skill': 0.4, 'text': 0.4, 'rating': 0.2}\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        top_5_indices = np.argsort(final_scores)[-5:][::-1]  # reverse for descending order\n",
    "        print(\"\\nTop 5 Matches:\")\n",
    "        for rank, idx in enumerate(top_5_indices, 1):\n",
    "            print(f\"{rank}. Freelancer {idx+1} with score {final_scores[idx]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to compute final scores: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
